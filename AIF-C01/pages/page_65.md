## Question #65

 A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt.

Which adjustment to an inference parameter should the company make to meet these requirements?

- A. Decrease the temperature value.

- B. Increase the temperature value.

- C. Decrease the length of output tokens.

- D. Increase the maximum generation length.