## Question #95

Which prompting technique can protect against prompt injection attacks?

- A. Adversarial prompting
- B. Zero-shot prompting
- C. Least-to-most prompting
- D. Chain-of-thought prompting 

Correct Answer: 
A Community vote distribution A (88%) D (12%)