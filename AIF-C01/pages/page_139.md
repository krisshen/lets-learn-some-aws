## Question #139

 A companyâ€™s large language model (LLM) is experiencing hallucinations.

How can the company decrease hallucinations?

- A. Set up Agents for Amazon Bedrock to supervise the model training.

- B. Use data pre-processing and remove any data that causes hallucinations.

- C. Decrease the temperature inference parameter for the model.

- D. Use a foundation model (FM) that is trained to not hallucinate.